{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3fa661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['GOOGLE_GEMINI_API_KEY'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5417bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai \n",
    "from google.genai import types \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd62a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "if not api_key :\n",
    "    raise ValueError('api key is not found ')\n",
    "\n",
    "client = genai.Client(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de52d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self notes :-  \n",
    "# gemini is very efficient in working with video . making it possible to do that used to need special, separate models . \n",
    "# with gemini vision abilities , you can : \n",
    "#  describe whats happening in the videos \n",
    "#  summarise and form notes out of videos \n",
    "#  split videos into parts and pull out key information \n",
    "#  query answering \n",
    "#  poin to exact moment \n",
    "\n",
    "# how that all possible  the capabilty of this native vision ?\n",
    "# when researcher developing the gemini familty then they have clear vision we have to build the product or llm which multimodel ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71432e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previos system working \n",
    "# there is three component : \n",
    "            # 1.visual interpretion \n",
    "            # 2. audio interpretion\n",
    "            # 2. processing text\n",
    "# all components are working saperately and then llm interpret this components output \n",
    "# the problem with that components is stiching (assume the video is containing image on the second of 1:00 to 1:05  and the voice for that perticular image is from 1:02 to 1:07. )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d69f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini's Approach (Built-in): Gemini was designed and trained from the ground up on different types of data (text, images, audio, video) all mixed \n",
    "# together. It doesn't have a separate \"brain\" for each sense. It has one unified model that learned the fundamental relationships between words, \n",
    "# pixels, and sounds simultaneously. This allows for a much deeper and more nuanced understanding\n",
    "# uniform brain capability diffrent from distributed component \n",
    "# other models are ignoring the low frames but the gemini try to improve these low frames (high quality video input)\n",
    "# native multimodaity , long context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b763faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file_name='GORILLA.mp4'\n",
    "video_bytes = open(video_file_name,'rb').read()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.5-flash',\n",
    "    contents = types.Content(\n",
    "        parts = [types.Part(\n",
    "            inline_data = types.Blob(data = video_bytes , mime_type ='video/mp4')\n",
    "        ),\n",
    "        types.Part(text = 'please summarize the video in three sentences. ')\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# catching in official document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1ca708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large gorilla stands on the grassy bank of a wide river, surrounded by lush trees. The gorilla turns to the viewer, vocalizing \"Hello there friends\" and waving. It then remarks on the beautiful day before turning its attention to a group of people visible on the far bank of the river, appearing to continue its conversation with them.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b63e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#request size(file, text, prompt , system instructions ) < 20 mb -> inline parameter method \n",
    "# request size > 20 mb we use file api to upload the data -> client.files.upload method  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d779a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = client.files.upload(file=\"videoplayback.mp4\")\n",
    "\n",
    "prompt = \"give me detail summary of the file \"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b56b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACTIVE'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.files.get(name = my_file.name).state.name  #when its return active then we can run  response code else we got an error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b287e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.5-flash' ,\n",
    "    contents =[my_file , prompt ],\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"please  give me output such that you are human\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b5ab52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video begins with the speaker, CA Rachana Ranade, introducing an IPO summary video for LG Electronics India Limited. She starts by asking a trivia question about the full form of LG, revealing it was \"Lucky Goldstar\" and later became simply \"LG,\" with \"Life's Good\" being the company's slogan.\n",
      "\n",
      "She then shares an update about her \"Har Ghar Investor\" Hyderabad event, extending an early bird offer due to high demand. She highlights key discussion points for the seminar, including fundamental analysis with AI tools, her Nifty target, new investment opportunities, and whether â‚¹1 crore is enough for retirement. The seminar will also feature a Q&A session.\n",
      "\n",
      "Ranade then delves into LG Electronics India Limited's operations, explaining that the company manufactures and sells home appliances (refrigerators, washing machines, air conditioners, water purifiers, dishwashers, microwave ovens, compressors, motors) and home entertainment products (televisions, audio devices, monitors, projectors, information displays). She clarifies that they operate in both B2C (Business-to-Consumer) and B2B (Business-to-Business) segments.\n",
      "\n",
      "Analyzing the revenue split for 2025, she shows that home appliances and air solutions contribute approximately 75% of the revenue, while home entertainment accounts for about 25%. Within home appliances, refrigerators, washing machines, and air conditioners each contribute roughly 20-27%, indicating a diversified product portfolio.\n",
      "\n",
      "Regarding geographical revenue, 94% comes from domestic sales, with a small portion from Africa and negligible sales from America and Europe. This means the company is largely unaffected by US tariffs.\n",
      "\n",
      "She then discusses LG's manufacturing and distribution presence in India, noting two manufacturing units in Noida and Pune, supported by two central distribution centers, 23 regional distribution centers, 51 branch offices, and over 30,000 sub-dealers. The production volume and capacity utilization are higher in Noida than in Pune, suggesting potential future capital expenditure if demand continues to rise.\n",
      "\n",
      "Moving on to key risks, Ranade identifies:\n",
      "1. **Raw Material Costs:** Purchases of raw materials constitute about 75% of the company's revenue from operations, making profitability vulnerable to raw material price increases.\n",
      "2. **Manufacturing Facilities:** Having only two manufacturing units (Noida and Pune) poses a risk; any disruptions or stoppages at these facilities could adversely impact the business.\n",
      "3. **Market Share:** LG's market share in key product categories (refrigerators, washing machines, air conditioners, televisions) has shown a continuous decrease from 2022 to 2025. This, combined with the highly competitive landscape in the electronics and appliances market, is a significant concern.\n",
      "\n",
      "Despite a growing industry (7% CAGR from 2019-2024, projected to accelerate to 11% CAGR from 2024-2029) and positive financial growth (revenue CAGR of 10.67%, EBITDA CAGR of 28.1%, PAT CAGR of 27.84% from FY22-23 to FY24-25), the declining market share is a red flag.\n",
      "\n",
      "Finally, she addresses the valuation, comparing LG's post-IPO P/E ratio of 37.6 with industry peers. While LG boasts higher revenue from operations, better EBITDA, PAT, ROCE, and RONW compared to many peers, its P/E ratio is lower than the industry average (56.49) and even lower than the highest peer (65.59). However, it's higher than the lowest peer (43.53). This implies that, based on current metrics, the company might be considered undervalued compared to the broader industry.\n",
      "\n",
      "The IPO, entirely an Offer for Sale (OFS) of â‚¹11,607.01 crores (meaning no funds will go to the company, only to selling shareholders), is open from October 7-9, 2025, with a price band of â‚¹1,080 to â‚¹1,140 per share.\n",
      "\n",
      "Ranade concludes by asking viewers if they plan to apply for the IPO and reminds them to like and subscribe to her channel. A disclaimer about the educational nature of her content and a warning against fraudulent stock tip advertisements follow.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74785ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also directly pass the url of the videos like youtube and other platform \n",
    "\n",
    "url_link = 'https://youtu.be/TzGtJfbCGws?si=Ki7HOF6JXnaFmhZP'\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.5-flash',\n",
    "    contents = types.Content(\n",
    "        parts = [types.Part(\n",
    "            file_data = types.FileData(file_uri = url_link )\n",
    "        ),\n",
    "        types.Part(text = 'please summarize the video ')\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "#here we just do some changes like in Filedata and pass the paramater uri ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd32fc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video is an AI news roundup presented by Vinayakk Singhal. Here's a summary of the major AI updates and their implications:\n",
      "\n",
      "**1. OpenAI's Sora 2 and its Valuation:**\n",
      "*   OpenAI has become the world's most valuable private company, valued at **$500 billion**, surpassing companies like SpaceX. This valuation is equivalent to the GDP of over 150 countries combined.\n",
      "*   The high valuation stems from a recent employee stock sale where $6.6 billion in shares were sold.\n",
      "*   OpenAI launched Sora 2, an AI video generator that creates videos with synchronized audio, accurate physics, and a built-in social app called Sora.\n",
      "*   Sora 2 allows users to insert themselves into AI-generated videos (cameos), fight dragons, do backflips, or even hang out in space.\n",
      "*   **Implications of Sora 2:**\n",
      "    *   **Marketing and Content Creation:** AI video tools, especially Sora 2, will dramatically reduce costs and time for product demos, ad campaigns, and fashion content by generating high-quality videos from text prompts without physical shoots.\n",
      "    *   **Filmmaking:** Directors can rapidly prototype entire scene concepts and test visual ideas before production, reducing the need for expensive storyboards and previews.\n",
      "    *   **Education:** Educators can visualize complex concepts, physics demonstrations, and historical recreations on demand. Medical researchers are already exploring its use for demonstrating surgical procedures.\n",
      "    *   **E-commerce:** Sellers can create professional-quality product shoots for impossible scenarios (e.g., pets using products, food sizzle reels, products in inaccessible environments) at a fraction of traditional costs.\n",
      "\n",
      "**2. Perplexity AI's Comet Browser:**\n",
      "*   Perplexity AI made its $200-per-month Comet browser entirely **free** worldwide.\n",
      "*   Comet is an AI-powered browser with a built-in assistant that can browse, compare, summarize, and even perform tasks like finding concert tickets and adding them to a cart.\n",
      "*   It also features a \"background assistant\" for Max plan subscribers, acting like a team of assistants to draft emails, find flights, and prepare meeting briefs simultaneously.\n",
      "*   **Implications:** This move challenges Google Chrome's dominance by competing on intelligence, not just speed, signaling the start of \"AI browser wars.\" Perplexity aims for market dominance rather than immediate profit.\n",
      "\n",
      "**3. Google DeepMind's Dreamer 4 in Minecraft:**\n",
      "*   Google DeepMind published a paper on Dreamer 4, an AI agent that learned to mine diamonds in Minecraft using **only offline data and imagination training**, without ever playing the actual game.\n",
      "*   Dreamer 4 watched 2500 hours of Minecraft videos, built an internal world model, and then practiced within its imagination to execute complex sequences of actions.\n",
      "*   It achieved diamond mining success in about 0.7% of its test runs, outperforming OpenAI's VTP offline agent, and runs smoothly on a single GPU.\n",
      "*   **Implications:** This is a significant step towards general-purpose robotics. Robots will be trained in simulated environments and imagination for millions of scenarios before executing tasks in reality, reducing the need for expensive real-world testing.\n",
      "\n",
      "**4. Anthropic's Claude Integration with Slack:**\n",
      "*   Anthropic integrated its Claude AI directly into Slack, making it accessible like a real person in any thread.\n",
      "*   Claude can prepare users for meetings by pulling relevant discussions, create documentation from scattered conversations, and draft responses to messages while respecting user permissions.\n",
      "*   For team and enterprise plans, Claude can summarize entire company Slack histories and integrate with Google Drive, Gmail, and Google Calendar.\n",
      "*   **Implications:** This will transform enterprise workflows, with some teams reporting a reduction in onboarding time from months to weeks.\n",
      "\n",
      "**5. Microsoft's \"Vibe Working\" with Office Apps:**\n",
      "*   Microsoft announced \"Vibe Working\" for Office apps.\n",
      "*   **Agent Mode in Excel** can run complete financial analyses, create visualizations, and validate results iteratively by simply understanding natural language commands (e.g., \"analyze sales data and make it visual\").\n",
      "*   **Agent Mode in Word** turns writing into conversations, handling tasks like updating monthly reports with new data, finding emails, updating tables, and fixing formatting.\n",
      "*   **Office Agent in Copilot** (for PowerPoint and Word) uses Anthropic's Claude to create documents, while Agent Mode in Excel uses OpenAI's models.\n",
      "*   **Implications:** Microsoft is hedging its bets with multiple AI providers. While not yet perfect, AI's accuracy in these tasks is rapidly improving, indicating a significant shift in office productivity.\n",
      "\n",
      "**6. Nothing's Essential Platform:**\n",
      "*   Nothing, the smartphone company, launched an AI platform that allows users to create apps by simply describing them.\n",
      "*   Users can describe functionalities like a wardrobe learning app or a receipt capture app, and the platform generates a \"widget\" for their home screen instantly.\n",
      "*   This platform, called the \"Nothing Playground,\" is a community hub where users can share, remix, and download AI-generated apps.\n",
      "*   **Implications:** This challenges the traditional app store concept by enabling users to create custom apps based on their specific needs. Nothing's CEO views this as a step towards an entirely AI-native operating system by 2027.\n",
      "\n",
      "**7. Toyota's Woven City:**\n",
      "*   Toyota opened its **$10 billion Woven City** at the base of Mount Fuji in Japan, with 300 residents already living there.\n",
      "*   It's a real town built to test future technology in everyday life, not just in labs.\n",
      "*   The city has three types of streets: one for autonomous vehicles, one for personal mobility (e-bikes), and one for pedestrians. Deliveries run through underground tunnels.\n",
      "*   Residents (mostly Toyota employees and researchers) are testing self-driving shuttles, delivery robots, smart homes with AI health monitoring, and hydrogen-powered infrastructure.\n",
      "*   **Implications:** This project is less about immediate profit and more about learning how humans interact with autonomous systems in real-life settings. This data is priceless for future AI and robotics development, making Woven City the first place where people can truly live in the future being tested.\n",
      "\n",
      "**Rapid-Fire Updates:**\n",
      "*   **NotebookLM** got customizable chats and a learning guide feature to test understanding of materials.\n",
      "*   **Notion** launched a map view for organizing projects spatially.\n",
      "*   **XAI** is building \"Grokipedia,\" a Wikipedia-like platform powered by Grok AI for real-time, up-to-date information.\n",
      "*   **Google Researchers** released \"PASTA,\" an AI system that learns individual creative preferences through repeated interactions, reducing the need for complex prompt engineering.\n",
      "*   **Tencent's** open-source Hunyuan Image 3.0 moved to first place on LM Arena's text-to-image leaderboard, surpassing top closed-source options.\n",
      "*   **Anthropic** published research detailing Claude 4.5 Sonnet's cybersecurity capabilities, achieving top marks on industry benchmarks.\n",
      "*   **OpenAI** acquired personal finance startup **Roi**, with its CEO joining as an AI leader.\n",
      "\n",
      "**Conclusion:**\n",
      "*   All these updates happened in **one week**, not one quarter or one year.\n",
      "*   We are witnessing the fastest wealth creation and technology deployment in human history.\n",
      "*   The global economy is reorganizing around AI at an unprecedented speed.\n",
      "*   **What you should do:**\n",
      "    1.  **Stop waiting for things to stabilize; they won't.** Companies investing heavily in AI are planning for acceleration, not slowdown.\n",
      "    2.  **Focus on learning patterns, not specific tools.** Tools will change, but understanding how AI thinks, its strengths, and how to direct it will be the lasting skill.\n",
      "    3.  **Your competitive advantage is not knowing more than AI**, but knowing what to build with AI that nobody else has thought of yet.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW LET SEE WHAT ALL TASK WE CAN DO WITH VIDEO UNDER-STANDING CAPABILITY OG GEMINI.\n",
    "    # Summarize the video.\n",
    "    # Answer questions about the video (KNOWLEDGE QUESTION, SCENE SEGMENTATION, OBJECT AND PERSON DETECTION).\n",
    "    # Speech-to-text (transcription).\n",
    "    # Emotion & sentiment detection.\n",
    "    # Data extraction from visuals :-Pull numbers, graphs, or chart information shown in frames.\n",
    "    # Multi-video comparison â†’ Compare two videos for similarities, differences, or changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e17a8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Answer questions about the video\n",
    "url_link = 'https://youtu.be/TzGtJfbCGws?si=Ki7HOF6JXnaFmhZP'\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.5-flash',\n",
    "    contents = types.Content(\n",
    "        parts = [types.Part(\n",
    "            file_data = types.FileData(file_uri = url_link )\n",
    "        ),\n",
    "        types.Part(text = 'give me the detail latest news about perplexity in the given video ')\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eacf9ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the video, here's the latest news about Perplexity AI:\n",
      "\n",
      "**Perplexity's Comet Browser is Now Free Worldwide**\n",
      "\n",
      "*   **Major Announcement:** Perplexity AI has made its Comet browser completely free for everyone worldwide. It was previously priced at $200 per month for \"Perplexity Max\" subscribers.\n",
      "*   **Key Features of Comet:**\n",
      "    *   It's an AI-powered web browser with a built-in assistant that can actively browse the web for you.\n",
      "    *   It can search, compare, and summarize information (e.g., compare insurance plans, find concert tickets and add them to a cart).\n",
      "    *   For former $200/month Max subscribers, there's a new \"background assistant\" feature. This assistant can handle multiple tasks simultaneously, such as drafting emails, finding the best flights, and preparing meeting briefs, all in the background.\n",
      "*   **Impact and Strategic Implications:**\n",
      "    *   The CEO claims this move could boost productivity so significantly that companies might not need to hire additional people.\n",
      "    *   Initial user data showed a massive increase in engagement: users who downloaded Comet increased their question asking by 6 to 18 times on day one.\n",
      "    *   This is seen as the beginning of \"AI browser wars,\" where competition shifts from just speed to intelligence.\n",
      "    *   By making a premium product free, Perplexity is signaling a strategy aimed at market dominance rather than immediate profit.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6099a854",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Speech to text(trancription)\n",
    "url_link = 'https://youtu.be/TzGtJfbCGws?si=Ki7HOF6JXnaFmhZP'\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.5-flash',\n",
    "    contents = types.Content(\n",
    "        parts = [types.Part(\n",
    "            file_data = types.FileData(file_uri = url_link )\n",
    "        ),\n",
    "        types.Part(text = 'Transcribe the complete spoken dialogue from this video please give me with timestamp .')\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54eb8315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00 - 00:04 OpenAI has just become the world's most valuable private company\n",
      "00:04 - 00:06 valued at $500 billion.\n",
      "00:06 - 00:09 worth more than the GDP of 150 countries combined.\n",
      "00:10 - 00:13 Perplexity made their $200 browser completely free.\n",
      "00:13 - 00:15 OpenAI also dropped Sora 2.\n",
      "00:15 - 00:19 A social app with use cases not restricted to socializing.\n",
      "00:19 - 00:21 Microsoft brought out Vibe working.\n",
      "00:21 - 00:26 And Google's AI just learned to mine diamonds in Minecraft without ever playing the game.\n",
      "00:26 - 00:27 And there is much much more.\n",
      "00:27 - 00:30 Hello everyone, I'm Vinayakk Singhal, and you're watching.\n",
      "00:30 - 00:33 The Cutting Edge News. From new breakthroughs to bold experiments.\n",
      "00:33 - 00:37 Here are 12 AI updates shaping the world's biggest headlines.\n",
      "00:37 - 00:39 And trust me, you need to know them all.\n",
      "00:39 - 00:41 It's just been a week and the most crazy one.\n",
      "00:41 - 00:45 So without any further delay, let's start with OpenAI and Sora 2.\n",
      "00:45 - 00:49 Today, we're announcing the Sora app, powered by the all-new Sora 2.\n",
      "00:49 - 00:54 Well, that may look and sound just like OpenAI CEO Sam Altman,\n",
      "00:54 - 00:58 but it's actually a video generated completely by AI.\n",
      "00:58 - 01:00 It's all powered by Sora 2.\n",
      "01:00 - 01:05 Now the clips are going viral, like one of CEO Sam Altman shoplifting GPUs.\n",
      "01:06 - 01:09 Please, I really need this for Sora inference. This video is too good.\n",
      "01:11 - 01:17 For those of you who missed the launch of Sora 2, it's the first AI video generator that creates videos with synchronized audio,\n",
      "01:17 - 01:20 accurate physics and a built-in social app.\n",
      "01:20 - 01:23 Like TikTok, every video is AI generated.\n",
      "01:23 - 01:25 The standout feature here is Cameos.\n",
      "01:25 - 01:31 You record yourself once, verify your identity and then you can insert yourself into any AI scene.\n",
      "01:31 - 01:36 Fighting dragons, doing backflips, hanging out in the space with your friends, everything's possible.\n",
      "01:36 - 01:39 But here's what this technology enables across different domains.\n",
      "01:39 - 01:42 First, marketing and content creation is being transformed.\n",
      "01:42 - 01:48 Early AI video tools have already shown dramatic productions and Sora 2 takes this further.\n",
      "01:48 - 01:56 Product demos that traditionally needed full camera crews, location shoots and editing teams can now be generated from text prompts.\n",
      "01:56 - 02:04 The physics accuracy means you can show products in motion, fabric flowing, liquids pouring, textures responding to light, all without a physical shoot.\n",
      "02:04 - 02:07 Second, filmmakers are using it for rapid prototyping.\n",
      "02:07 - 02:15 Instead of expensive previews or storyboards, directors can now generate entire scene concepts to test visual ideas before committing to production.\n",
      "02:15 - 02:22 Want to see how a zero gravity fight sequence might look, or a rain scene with specific lighting, generate it in minutes.\n",
      "02:22 - 02:24 Iterate, then shoot the real thing with confidence.\n",
      "02:24 - 02:28 Third, educators have a new visualization tool.\n",
      "02:28 - 02:38 Complex concepts that were previously limited to diagrams or expensive animations can now be brought to life, physics demonstrations, historical recreations, all can be generated on demand.\n",
      "02:38 - 02:46 Medical education researchers have already published papers exploring how the original Sora could demonstrate surgical procedures without operating rooms.\n",
      "02:46 - 02:50 Fourth, E-commerce sellers can solve impossible product shoots.\n",
      "02:50 - 02:59 Need to show a pet product in action, but your cat won't cooperate? Want food sizzle reels without waste? Need to demonstrate product in environments you can't access?\n",
      "02:59 - 03:02 AI video generation removes these physical constraints.\n",
      "03:02 - 03:07 Small businesses can now create professional quality content at a fraction of traditional costs.\n",
      "03:07 - 03:09 Now coming to the insane part.\n",
      "03:09 - 03:16 While OpenAI was launching Sora 2, it officially became the most valuable private company in the world at $500 billion valuation.\n",
      "03:16 - 03:17 Let me put that in perspective.\n",
      "03:17 - 03:22 SpaceX, the number two most valuable private company, is valued at $400 billion.\n",
      "03:22 - 03:25 OpenAI just overtook them.\n",
      "03:25 - 03:34 This happened after a recent employee stock sale where the company authorized employees to sell $10 billion in shares. Though shares worth only $6.6 billion were ultimately sold.\n",
      "03:34 - 03:36 But here's the wild part.\n",
      "03:36 - 03:41 That valuation means OpenAI is worth more than the GDP of 150 countries combined.\n",
      "03:41 - 03:42 Think about that for a second.\n",
      "03:42 - 03:49 A company that's less than 10 years old that makes a chatbot and some AI tools is now worth half a trillion dollars.\n",
      "03:49 - 03:59 OpenAI will reshape every industry, every workflow, every creative process. And based on what we are seeing with Sora 2 and the other updates this week, they might just be right.\n",
      "03:59 - 04:02 Speaking of massive moves, Perplexity just made one.\n",
      "04:02 - 04:06 Remember their Comet browser that cost $200 per month?\n",
      "04:06 - 04:08 They made it completely free.\n",
      "04:08 - 04:13 Comet is an AI powered browser with a built-in assistant that can actually browse for you.\n",
      "04:13 - 04:17 Need to compare insurance plans? Comet searches, compares and summarizes.\n",
      "04:17 - 04:21 Want to find the cheapest concert tickets? Comet adds them to your cart while you make a sandwich.\n",
      "04:21 - 04:27 The CEO said this can boost productivity so much that companies won't need to hire additional people.\n",
      "04:27 - 04:29 That's a bold claim, but here's what's real.\n",
      "04:29 - 04:34 Users who downloaded Comet increased their question asking by 6 to 18x on day one.\n",
      "04:34 - 04:43 For subscribers of the Max plan, which costs $200 a month, there's now a background assistant feature that works like having a team of assistants.\n",
      "04:43 - 04:49 You can tell it to draft your emails, find the best flights and prepare a meeting brief, all simultaneously in the background.\n",
      "04:49 - 04:52 Why does this matter?\n",
      "04:52 - 05:01 Google Chrome has dominated browsers for over a decade. Now we have AI-first browsers competing on intelligence, not just speed. This is the beginning of AI browser wars. And when a company makes a $200 product free,\n",
      "05:01 - 05:04 they're planning for market dominance, not profit.\n",
      "05:04 - 05:08 Now here is where things get really interesting.\n",
      "05:08 - 05:13 Google DeepMind just published a paper on Dreamer 4, and this is genuinely groundbreaking.\n",
      "05:13 - 05:22 Dreamer 4 became the first AI agent to mine diamonds in Minecraft using only offline data. It never played the actual game during training. Instead, it learned by imagining.\n",
      "05:22 - 05:24 Think about what this means.\n",
      "05:24 - 05:36 The AI watched 2,500 hours of Minecraft videos, built an internal world model, then practiced inside its own imagination until it could execute 20,000 plus consecutive actions to reach diamonds.\n",
      "05:36 - 05:40 Dreamer 4 managed to mine diamonds in about 0.7% of its test runs.\n",
      "05:40 - 05:52 That might sound small, but it's still ahead of OpenAI's VPT agent, an earlier AI trained on thousands of hours of Minecraft videos to imitate human players, which could not reach that milestone.\n",
      "05:52 - 05:57 Even more impressive, Dreamer 4 runs smoothly on a single GPU, far faster than older models.\n",
      "05:57 - 06:02 And this matters beyond gaming because this is exactly how we'll train robots in the future.\n",
      "06:02 - 06:78 Instead of expensive real world testing, robots will practice millions of scenarios in imagination, then execute in reality.\n",
      "06:07 - 06:11 This is the path to general purpose robotics.\n",
      "06:11 - 06:15 Anthropic just integrated Claude directly into Slack.\n",
      "06:15 - 06:17 This is a huge news for enterprise workflows.\n",
      "06:17 - 06:24 You can now mention Claude in any Slack thread as if a real person, DM it privately or use the AI assistant panel.\n",
      "06:24 - 06:33 Coming to the smart part, they also let you connect Slack to Claude, so when you're doing deep research in Claude, it can pull context from your Slack conversations and honestly, its use cases are wild.\n",
      "06:33 - 06:38 First, it can prepare you for meetings by pulling relevant discussions.\n",
      "06:38 - 06:44 Second, it can create documentation from scattered conversations. And third, it can even draft your responses to important messages.\n",
      "06:44 - 06:49 All while respecting your permission settings because it can only see what you see.\n",
      "06:49 - 06:53 For team and enterprise plans, this will transform how companies work.\n",
      "06:53 - 07:00 One team reported cutting onboarding time from months to weeks because Claude could instantly summarize the entire company's Slack history or any topic.\n",
      "07:00 - 07:03 Microsoft just announced Vibe Working.\n",
      "07:03 - 07:06 Their answer to Vibe coding, but only for office apps.\n",
      "07:06 - 07:13 Agent Mode in Excel can now run complete financial analysis, create visualizations and validate results iteratively.\n",
      "07:13 - 07:20 You just say analyze the sales data and make it visual. And it decides which formulas to use, creates new sheets and even builds charts.\n",
      "07:20 - 07:24 In Microsoft Word, Agent Mode turns writing into conversation.\n",
      "07:24 - 07:34 Tell it, update this monthly report into latest data from the September email and it handles everything, finding the email, updating tables and even fixing the format.\n",
      "07:34 - 07:36 But here is the twist.\n",
      "07:36 - 07:43 The office agent in Copilot uses Anthropic's Claude to create PowerPoint and Word docs. While Agent Mode uses OpenAI's models.\n",
      "07:43 - 07:46 Microsoft is hedging its best with multiple AI providers.\n",
      "07:46 - 07:54 The accuracy, Agent Mode scored 57.2% on spreadsheet benchmarks compared to 71.3% for human beings.\n",
      "07:54 - 07:56 Not perfect, but improving fast.\n",
      "07:56 - 08:00 Nothing's Essential platform just dropped and this is wild.\n",
      "08:00 - 08:11 The smartphone company launched an AI platform that lets you create apps just by describing them. Want an app that captures receipts from your camera roll and exports a finance ready PDF every Friday, just say it.\n",
      "08:11 - 08:17 Want to mood tracker that syncs with the music playlist, done, it generates these as widgets you can add to your home screen instantly.\n",
      "08:17 - 08:24 And they're calling it the Nothing Playground, a community hub where you can share, remix and download apps created by others.\n",
      "08:24 - 08:32 CEO Carl Pei says that this is the first step towards building an entirely AI native operating system by 2027.\n",
      "08:32 - 08:41 Currently available only on Nothing phones, but this challenges the entire concept of App stores. Why download pre-made apps when AI can build exactly what you need?\n",
      "08:41 - 08:43 Another very interesting news.\n",
      "08:43 - 08:52 Toyota just opened its $10 billion Woven City at the base of Mount Fuji in Japan. And 300 residents are already living there as of October 2025.\n",
      "08:52 - 08:57 This is not just a simulation, it's a real town built to test future technology in everyday life.\n",
      "08:57 - 09:06 The streets are split into three zones, one for autonomous vehicles, one for personal mobility like e-bikes and one for purely pedestrians.\n",
      "09:06 - 09:10 All deliveries run through underground tunnels carved into volcanic rocks.\n",
      "09:10 - 09:20 Residents, mostly Toyota employees and researchers are testing self-driving shuttles, delivery robots, smart homes with AI health monitoring and hydrogen powered everything.\n",
      "09:20 - 09:28 But here's the key, it's not just about the tech, it's about how homes, mobility, logistics and energy systems connect and evolve based on how people actually live.\n",
      "09:28 - 09:32 Toyota invested $10 billion knowing this might never make them money.\n",
      "09:32 - 09:40 But they're betting that learning how humans interact with autonomous systems in real life, not in labs, but while living, walking, working, that data is priceless.\n",
      "09:40 - 09:47 This is the first place where you can actually live in the future being tested, not on paper, but in reality.\n",
      "09:47 - 09:52 Just thinking about living in such a place in India gives me goosebumps.\n",
      "09:52 - 09:54 Tell me what you think about it.\n",
      "09:54 - 09:58 Now, it's time to shift gears for a quick rapid fire round up of this week's biggest AI updates.\n",
      "09:58 - 10:04 NotebookLM got customizable chats and a learning guide feature which will test and deepen your understanding of your materials.\n",
      "10:04 - 10:08 Notion launched map view, finally a spatial way to organize your projects.\n",
      "10:08 - 10:15 xAI is building Grokipedia, basically Wikipedia powered by Grok AI for real-time up-to-date information.\n",
      "10:15 - 10:27 Google researchers released Pasta, an AI system that adapts to individual creative preferences through repeated interactions, learning what visual styles users actually want rather than requiring complex prompt engineering.\n",
      "10:27 - 10:36 Tencent open sourced Hunyuan Image 3.0 model move to first place on LMArena's text to image leaderboard, surpassing top closed options.\n",
      "10:36 - 10:45 Anthropic published research detailing Claude 4.5 Sonnet's cyber security capabilities with the new model achieving top marks on industry benchmarks.\n",
      "10:45 - 10:51 OpenAI acquired personal finance startup Roy with CEO Sujit Vishwajit joining as the AI leader in the deal.\n",
      "10:51 - 10:56 Now here's what connects all of this. We're witnessing the fastest wealth creation in human history alongside the fastest technology deployment.\n",
      "10:56 - 11:01 OpenAI hits $500 billion valuation.\n",
      "11:01 - 11:04 They use that to build Sora 2. Sora 2 creates a new social platform.\n",
      "11:04 - 11:10 Now we're moving from AI as a tool to AI as a collaborator to AI as an autonomous worker.\n",
      "11:10 - 11:19 And here's what nobody is talking about. Every single one of these updates happened in one week, not one quarter, not one year, one week.\n",
      "11:19 - 11:34 When a technology company becomes worth more than most countries economies in less than a decade and when that same week they release tools that can imagine, browse, collaborate and create autonomously, we're not watching incremental progress anymore, we're watching exponential transformation.\n",
      "11:34 - 11:36 So what should you do?\n",
      "11:36 - 11:46 First, stop waiting for things to stabilize. They won't. The companies betting half a trillion dollars on AI aren't playing for this to slow down. They're planning for it to accelerate.\n",
      "11:46 - 11:57 Second, focus on learning patterns, not specific tools. The tools will change every now and then, but understanding how AI thinks, what it's good at, and how to direct it, that's the lasting skill.\n",
      "11:57 - 12:10 Third, remember when AI makes a company worth $500 billion and enables tools that work autonomously, your competitive advantage isn't knowing more than AI. It's knowing what to build with AI that nobody else has thought of yet.\n",
      "12:10 - 12:18 This week, OpenAI became the world's most valuable private company at $500 billion valuation, worth more than 150 countries combined.\n",
      "12:18 - 12:21 They launched Sora 2 with social features and accurate physics.\n",
      "12:21 - 12:23 Perplexity made Comet browser free.\n",
      "12:23 - 12:26 Dreamer 4 mastered Minecraft through imagination.\n",
      "12:26 - 12:28 Claude integrated with Slack.\n",
      "12:28 - 12:30 Microsoft introduced Vibe Working.\n",
      "12:30 - 12:33 And about six other major updates. The pattern, we're not just seeing AI improve.\n",
      "12:33 - 12:40 We're seeing the entire global economy reorganized around it at unprecedented speed.\n",
      "12:40 - 12:44 This is Vinayakk and you're watching The Cutting Edge News.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5643b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emaotion and sentiment detection \n",
    "url_link = 'https://youtu.be/TzGtJfbCGws?si=Ki7HOF6JXnaFmhZP'\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.5-flash',\n",
    "    contents = types.Content(\n",
    "        parts = [types.Part(\n",
    "            file_data = types.FileData(file_uri = url_link )\n",
    "        ),\n",
    "        types.Part(text = 'analyze the video and describe the emotion or moods in each scenes , based on tone of voice and facial expressions ')\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2342f6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will provide a detailed analysis of the emotions and moods conveyed in the video, focusing on the speaker's tone of voice and facial expressions.\n",
      "\n",
      "Here's a breakdown of the observed emotions and moods:\n",
      "\n",
      "**00:00 - 00:03**: The speaker's voice is informative and slightly enthusiastic, with a neutral facial expression. The mood is **informative and intriguing**.\n",
      "\n",
      "**00:03 - 00:06**: The voice remains informative, and the pace quickens slightly to emphasize the valuation. The facial expression is neutral. The mood is **factual and impressive**.\n",
      "\n",
      "**00:06 - 00:09**: The voice takes on a more serious and impactful tone to convey the significant comparison. The facial expression remains neutral. The mood is **serious and impactful**.\n",
      "\n",
      "**00:09 - 00:13**: The speaker's voice is upbeat and slightly surprised, with a subtle raising of eyebrows. The mood is **surprised and positive**.\n",
      "\n",
      "**00:13 - 00:16**: The voice becomes more excited and slightly dramatic to announce the new release. The facial expression shows anticipation and excitement. The mood is **excited and anticipatory**.\n",
      "\n",
      "**00:16 - 00:19**: The voice is calm and informative, listing functionalities. The facial expression is neutral. The mood is **explanatory and neutral**.\n",
      "\n",
      "**00:19 - 00:26**: The voice is enthusiastic and engaging, with a slight upward inflection at the end of sentences. The facial expression shows interest and engagement. The mood is **engaging and curious**.\n",
      "\n",
      "**00:26 - 00:29**: The speaker's voice is warm and welcoming, with a friendly smile. The mood is **friendly and inviting**.\n",
      "\n",
      "**00:29 - 00:33**: The voice is energetic and excited, with a broad smile. The mood is **dynamic and enthusiastic**.\n",
      "\n",
      "**00:33 - 00:37**: The voice is confident and engaging, with a direct gaze. The mood is **confident and intriguing**.\n",
      "\n",
      "**00:37 - 00:41**: The voice is emphatic and urgent, with a serious but engaged facial expression. The mood is **urgent and captivating**.\n",
      "\n",
      "**00:41 - 00:45**: The voice is direct and decisive, with a clear and confident tone. The mood is **decisive and eager**.\n",
      "\n",
      "**00:45 - 00:50**: The speaker's tone is confident and clear, with a slight smile, conveying the announcement. The mood is **professional and positive**.\n",
      "\n",
      "**00:50 - 00:53**: The tone is excited and slightly exaggerated, with wide eyes and a smile, reflecting the surprising nature of the video. The mood is **amazed and playful**.\n",
      "\n",
      "**00:53 - 00:56**: The tone is still excited but more composed, with a slight grin, indicating the impressive technology. The mood is **impressed and forward-looking**.\n",
      "\n",
      "**00:56 - 00:59**: The tone is enthusiastic and celebratory, emphasizing the AI's capability. The facial expression is animated. The mood is **celebratory and exciting**.\n",
      "\n",
      "**00:59 - 01:02**: The tone is serious and slightly awe-struck, highlighting the advanced nature of Sora 2. The facial expression is one of wonder. The mood is **awe-struck and serious**.\n",
      "\n",
      "**01:02 - 01:09**: The tone is slightly humorous and self-deprecating, as the speaker references a comical situation. The facial expression is amused. The mood is **humorous and lighthearted**.\n",
      "\n",
      "**01:09 - 01:13**: The speaker's voice is calm and explanatory, with a neutral and informative facial expression. The mood is **informative and neutral**.\n",
      "\n",
      "**01:13 - 01:18**: The tone is impressed and slightly amazed, highlighting the technical advancements. The facial expression is one of wonder. The mood is **impressed and awe-struck**.\n",
      "\n",
      "**01:18 - 01:22**: The voice is enthusiastic and slightly playful, showcasing diverse applications. The facial expression is animated and engaged. The mood is **engaging and versatile**.\n",
      "\n",
      "**01:22 - 01:25**: The voice is confident and explanatory, introducing the standout feature. The facial expression is focused and clear. The mood is **confident and informative**.\n",
      "\n",
      "**01:25 - 01:36**: The voice is excited and almost breathless, detailing the potential of cameos. The facial expression shows excitement and wonder. The mood is **enthusiastic and imaginative**.\n",
      "\n",
      "**01:36 - 01:39**: The voice is confident and assertive, setting the stage for discussing applications. The facial expression is direct and determined. The mood is **assertive and forward-looking**.\n",
      "\n",
      "**01:39 - 01:42**: The voice is clear and precise, introducing the first domain transformation. The facial expression is serious and thoughtful. The mood is **analytical and serious**.\n",
      "\n",
      "**01:42 - 01:56**: The voice is detailed and explanatory, highlighting the benefits of AI in content creation. The facial expression is engaged and precise. The mood is **explanatory and focused**.\n",
      "\n",
      "**01:56 - 02:03**: The voice is confident and assured, emphasizing the physical accuracy of the AI. The facial expression is precise and serious. The mood is **confident and detailed**.\n",
      "\n",
      "**02:03 - 02:07**: The voice is clear and slightly dramatic, introducing the next domain. The facial expression is serious and thoughtful. The mood is **informative and impactful**.\n",
      "\n",
      "**02:07 - 02:12**: The voice is enthusiastic and visionary, describing the benefits for filmmakers. The facial expression is engaged and bright. The mood is **visionary and positive**.\n",
      "\n",
      "**02:12 - 02:20**: The voice is excited and imaginative, detailing the possibilities for scene creation. The facial expression shows excitement and creativity. The mood is **creative and enthusiastic**.\n",
      "\n",
      "**02:20 - 02:24**: The voice is practical and encouraging, emphasizing efficiency and confidence. The facial expression is confident and reassuring. The mood is **practical and supportive**.\n",
      "\n",
      "**02:24 - 02:28**: The voice is clear and slightly elevated, introducing the third domain. The facial expression is serious and impactful. The mood is **informative and significant**.\n",
      "\n",
      "**02:28 - 02:32**: The voice is enthusiastic and explanatory, detailing how AI brings concepts to life. The facial expression is engaged and bright. The mood is **engaging and illustrative**.\n",
      "\n",
      "**02:32 - 02:38**: The voice is amazed and almost reverent, highlighting the diverse educational applications. The facial expression is one of wonder. The mood is **awe-struck and visionary**.\n",
      "\n",
      "**02:38 - 02:46**: The voice is confident and proud, mentioning research in medical applications. The facial expression is serious and accomplished. The mood is **accomplished and credible**.\n",
      "\n",
      "**02:46 - 02:50**: The voice is clear and slightly excited, introducing the fourth domain. The facial expression is engaged and slightly surprised. The mood is **intriguing and promising**.\n",
      "\n",
      "**02:50 - 03:00**: The voice is enthusiastic and descriptive, illustrating how AI solves production challenges. The facial expression shows excitement and problem-solving. The mood is **innovative and practical**.\n",
      "\n",
      "**03:00 - 03:07**: The voice is confident and empowering, highlighting the accessibility for small businesses. The facial expression is encouraging and positive. The mood is **empowering and optimistic**.\n",
      "\n",
      "**03:07 - 03:10**: The voice becomes more dramatic and serious, preparing for a significant announcement. The facial expression is intense and focused. The mood is **dramatic and serious**.\n",
      "\n",
      "**03:10 - 03:16**: The voice is factual and impactful, announcing OpenAI's valuation. The facial expression is serious and conveys importance. The mood is **significant and factual**.\n",
      "\n",
      "**03:16 - 03:22**: The voice is calm and explanatory, putting the valuation into perspective. The facial expression is thoughtful. The mood is **analytical and clarifying**.\n",
      "\n",
      "**03:22 - 03:26**: The voice is assertive and triumphant, emphasizing OpenAI's overtaking of SpaceX. The facial expression is confident and proud. The mood is **triumphant and competitive**.\n",
      "\n",
      "**03:26 - 03:35**: The voice is detailed and explanatory, describing the stock sale. The facial expression is neutral and informative. The mood is **factual and business-oriented**.\n",
      "\n",
      "**03:35 - 03:41**: The voice becomes more dramatic and impactful, reiterating the comparison to national GDPs. The facial expression is serious and slightly wide-eyed. The mood is **awe-inspiring and profound**.\n",
      "\n",
      "**03:41 - 03:45**: The voice is intense and slightly incredulous, emphasizing the rapid growth and valuation. The facial expression is wide-eyed and slightly surprised. The mood is **amazed and reflective**.\n",
      "\n",
      "**03:45 - 03:59**: The voice is confident and visionary, predicting AI's transformative impact. The facial expression is determined and forward-looking. The mood is **visionary and impactful**.\n",
      "\n",
      "**03:59 - 04:02**: The voice shifts to an energetic and excited tone, with a bright smile, indicating a new topic. The mood is **energetic and anticipatory**.\n",
      "\n",
      "**04:02 - 04:08**: The voice is upbeat and slightly triumphant, announcing the free browser. The facial expression is positive and slightly playful. The mood is **positive and celebratory**.\n",
      "\n",
      "**04:08 - 04:13**: The voice is enthusiastic and descriptive, explaining the browser's capabilities. The facial expression is engaged and bright. The mood is **explanatory and exciting**.\n",
      "\n",
      "**04:13 - 04:21**: The voice is confident and highlights the browser's efficiency. The facial expression is focused and reassuring. The mood is **confident and practical**.\n",
      "\n",
      "**04:21 - 04:26**: The voice is calm and informative, quoting the CEO's bold claims. The facial expression is neutral. The mood is **factual and intriguing**.\n",
      "\n",
      "**04:26 - 04:34**: The voice becomes more emphatic and slightly challenging, stating what's \"real.\" The facial expression is serious and direct. The mood is **assertive and validating**.\n",
      "\n",
      "**04:34 - 04:42**: The voice is detailed and explanatory, describing the background assistant feature. The facial expression is informative and precise. The mood is **explanatory and insightful**.\n",
      "\n",
      "**04:42 - 04:49**: The voice is excited and imaginative, illustrating diverse tasks the assistant can handle. The facial expression shows excitement and wonder. The mood is **imaginative and positive**.\n",
      "\n",
      "**04:49 - 05:01**: The voice is serious and impactful, explaining the broader significance of AI browsers. The facial expression is thoughtful and analytical. The mood is **analytical and visionary**.\n",
      "\n",
      "**05:01 - 05:06**: The voice is confident and strategic, explaining the market dominance strategy. The facial expression is determined and insightful. The mood is **strategic and competitive**.\n",
      "\n",
      "**05:06 - 05:10**: The voice is serious and slightly dramatic, introducing a truly groundbreaking development. The facial expression is intense and focused. The mood is **dramatic and groundbreaking**.\n",
      "\n",
      "**05:10 - 05:17**: The voice is amazed and slightly incredulous, highlighting the unique learning method. The facial expression is wide-eyed and wondering. The mood is **awe-struck and insightful**.\n",
      "\n",
      "**05:17 - 05:22**: The voice is calm and explanatory, detailing the AI's training process. The facial expression is neutral and informative. The mood is **factual and technical**.\n",
      "\n",
      "**05:22 - 05:27**: The voice is enthusiastic and amazed, reiterating the imagination-based learning. The facial expression is wide-eyed and excited. The mood is **amazed and illustrative**.\n",
      "\n",
      "**05:27 - 05:35**: The voice is detailed and precise, explaining the internal world model and practice. The facial expression is focused and technical. The mood is **technical and explanatory**.\n",
      "\n",
      "**05:35 - 05:40**: The voice is confident and slightly triumphant, stating the success rate. The facial expression is proud and accomplished. The mood is **accomplished and confident**.\n",
      "\n",
      "**05:40 - 05:47**: The voice is comparative and highlights the AI's superior performance. The facial expression is serious and analytical. The mood is **analytical and competitive**.\n",
      "\n",
      "**05:47 - 05:52**: The voice is explanatory, illustrating the initial steps of the AI. The facial expression is neutral and informative. The mood is **factual and sequential**.\n",
      "\n",
      "**05:52 - 05:58**: The voice is impressed and emphasizes the efficiency of the new model. The facial expression is bright and positive. The mood is **impressed and efficient**.\n",
      "\n",
      "**05:58 - 06:03**: The voice is confident and visionary, extending the implications to robotics. The facial expression is determined and forward-looking. The mood is **visionary and impactful**.\n",
      "\n",
      "**06:03 - 06:08**: The voice is enthusiastic and describes the future of robot training. The facial expression shows excitement and wonder. The mood is **imaginative and progressive**.\n",
      "\n",
      "**06:08 - 06:12**: The voice is confident and emphasizes the practical application in robotics. The facial expression is determined and assured. The mood is **assertive and practical**.\n",
      "\n",
      "**06:12 - 06:16**: The voice is energetic and excited, announcing the Slack integration. The facial expression is bright and positive. The mood is **enthusiastic and promising**.\n",
      "\n",
      "**06:16 - 06:23**: The voice is enthusiastic and explanatory, detailing Claude's functionalities in Slack. The facial expression is engaged and precise. The mood is **explanatory and efficient**.\n",
      "\n",
      "**06:23 - 06:34**: The voice is confident and highlights the smart integration with Slack. The facial expression is focused and precise. The mood is **confident and insightful**.\n",
      "\n",
      "**06:34 - 06:49**: The voice is enthusiastic and almost breathless, detailing the powerful use cases. The facial expression shows excitement and wonder. The mood is **impressed and transformative**.\n",
      "\n",
      "**06:49 - 06:53**: The voice is confident and highlights the benefits for teams and enterprises. The facial expression is positive and forward-looking. The mood is **optimistic and impactful**.\n",
      "\n",
      "**06:53 - 07:00**: The voice is descriptive and illustrates the real-world benefits of the integration. The facial expression is engaged and precise. The mood is **practical and beneficial**.\n",
      "\n",
      "**07:00 - 07:05**: The voice is energetic and slightly playful, introducing \"Vibe Working.\" The facial expression is bright and curious. The mood is **playful and intriguing**.\n",
      "\n",
      "**07:05 - 07:08**: The voice is clear and concise, specifying the application scope. The facial expression is neutral. The mood is **factual and precise**.\n",
      "\n",
      "**07:08 - 07:21**: The voice is enthusiastic and describes the automation in Excel. The facial expression is engaged and amazed. The mood is **amazed and efficient**.\n",
      "\n",
      "**07:21 - 07:34**: The voice is excited and detailed, explaining the features in Word. The facial expression shows enthusiasm and precision. The mood is **detailed and revolutionary**.\n",
      "\n",
      "**07:34 - 07:45**: The voice becomes more strategic and analytical, revealing Microsoft's hedging strategy. The facial expression is thoughtful and insightful. The mood is **strategic and analytical**.\n",
      "\n",
      "**07:45 - 07:56**: The voice is serious and highlights the accuracy benchmarks. The facial expression is precise and objective. The mood is **factual and evaluative**.\n",
      "\n",
      "**07:56 - 08:00**: The voice is energetic and excited, introducing Nothing's platform. The facial expression is bright and positive. The mood is **enthusiastic and promising**.\n",
      "\n",
      "**08:00 - 08:10**: The voice is enthusiastic and almost breathless, detailing app creation by description. The facial expression shows excitement and wonder. The mood is **innovative and imaginative**.\n",
      "\n",
      "**08:10 - 08:17**: The voice is confident and highlights the ease of widget creation. The facial expression is positive and reassuring. The mood is **confident and user-friendly**.\n",
      "\n",
      "**08:17 - 08:24**: The voice is enthusiastic and engaging, describing the community platform. The facial expression is bright and inviting. The mood is **community-oriented and positive**.\n",
      "\n",
      "**08:24 - 08:31**: The voice is serious and visionary, quoting the CEO's ambition. The facial expression is thoughtful and determined. The mood is **visionary and strategic**.\n",
      "\n",
      "**08:31 - 08:38**: The voice is confident and emphasizes the challenge to app stores. The facial expression is direct and impactful. The mood is **assertive and disruptive**.\n",
      "\n",
      "**08:38 - 08:41**: The voice is rhetorical and challenging, questioning traditional app models. The facial expression is thoughtful and questioning. The mood is **thought-provoking and innovative**.\n",
      "\n",
      "**08:41 - 08:46**: The voice is energetic and excited, announcing Toyota's Woven City. The facial expression is bright and amazed. The mood is **amazed and futuristic**.\n",
      "\n",
      "**08:46 - 08:50**: The voice is enthusiastic and describes the initial residents. The facial expression is engaged and positive. The mood is **optimistic and groundbreaking**.\n",
      "\n",
      "**08:50 - 08:55**: The voice is confident and highlights the real-world application of the city. The facial expression is determined and visionary. The mood is **visionary and practical**.\n",
      "\n",
      "**08:55 - 09:06**: The voice is detailed and explanatory, describing the segregated streets. The facial expression is neutral and informative. The mood is **explanatory and organized**.\n",
      "\n",
      "**09:06 - 09:10**: The voice is amazed and slightly futuristic, describing the underground deliveries. The facial expression is wide-eyed and impressed. The mood is **futuristic and innovative**.\n",
      "\n",
      "**09:10 - 09:19**: The voice is confident and highlights the various technologies being tested. The facial expression is determined and visionary. The mood is **visionary and advanced**.\n",
      "\n",
      "**09:19 - 09:28**: The voice is serious and emphasizes the human interaction aspect of the tech. The facial expression is thoughtful and insightful. The mood is **insightful and human-centric**.\n",
      "\n",
      "**09:28 - 09:32**: The voice is serious and impactful, highlighting Toyota's massive investment. The facial expression is grave and significant. The mood is **serious and strategic**.\n",
      "\n",
      "**09:32 - 09:40**: The voice is confident and visionary, explaining the long-term bet. The facial expression is determined and forward-looking. The mood is **visionary and strategic**.\n",
      "\n",
      "**09:40 - 09:46**: The voice becomes more excited and almost breathless, emphasizing the unique living experience. The facial expression shows excitement and wonder. The mood is **exhilarated and futuristic**.\n",
      "\n",
      "**09:46 - 09:52**: The voice is slightly whimsical and reflective, expressing personal wonder. The facial expression is thoughtful and a subtle smile. The mood is **reflective and longing**.\n",
      "\n",
      "**09:52 - 09:54**: The voice is direct and engaging, inviting audience interaction. The facial expression is open and curious. The mood is **engaging and inquisitive**.\n",
      "\n",
      "**09:54 - 09:58**: The voice is energetic and fast-paced, signaling a rapid update segment. The facial expression is animated and quick. The mood is **fast-paced and exciting**.\n",
      "\n",
      "**09:58 - 10:04**: The voice is enthusiastic and descriptive, detailing NotebookLM's new features. The facial expression is bright and positive. The mood is **informative and positive**.\n",
      "\n",
      "**10:04 - 10:08**: The voice is excited and slightly playful, announcing Notion's map view. The facial expression is bright and positive. The mood is **innovative and practical**.\n",
      "\n",
      "**10:08 - 10:15**: The voice is enthusiastic and describes Grokipedia's real-time knowledge. The facial expression is engaged and bright. The mood is **informative and advanced**.\n",
      "\n",
      "**10:15 - 10:18**: The voice is excited and slightly dramatic, introducing Google PASTA. The facial expression is animated and intrigued. The mood is **dramatic and groundbreaking**.\n",
      "\n",
      "**10:18 - 10:27**: The voice is enthusiastic and descriptive, explaining PASTA's adaptive learning. The facial expression shows excitement and precision. The mood is **innovative and adaptive**.\n",
      "\n",
      "**10:27 - 10:36**: The voice is confident and slightly triumphant, announcing Tencent's achievement. The facial expression is proud and accomplished. The mood is **triumphant and competitive**.\n",
      "\n",
      "**10:36 - 10:44**: The voice is serious and highlights Anthropic's cyber security capabilities. The facial expression is focused and credible. The mood is **credible and advanced**.\n",
      "\n",
      "**10:44 - 10:50**: The voice is confident and clear, announcing OpenAI's acquisition. The facial expression is neutral and informative. The mood is **factual and strategic**.\n",
      "\n",
      "**10:50 - 10:55**: The voice becomes more reflective and serious, connecting the updates to wealth creation. The facial expression is thoughtful and profound. The mood is **reflective and impactful**.\n",
      "\n",
      "**10:55 - 10:57**: The voice is serious and emphasizes the speed of technological deployment. The facial expression is grave and significant. The mood is **serious and accelerating**.\n",
      "\n",
      "**10:57 - 11:01**: The voice is impactful and confident, highlighting OpenAI's valuation. The facial expression is direct and important. The mood is **authoritative and significant**.\n",
      "\n",
      "**11:01 - 11:05**: The voice is enthusiastic and emphasizes the social platform created. The facial expression is bright and visionary. The mood is **visionary and positive**.\n",
      "\n",
      "**11:05 - 11:11**: The voice is analytical and insightful, describing the shift in AI's role. The facial expression is thoughtful and clear. The mood is **analytical and transformative**.\n",
      "\n",
      "**11:11 - 11:19**: The voice is dramatic and intense, emphasizing the rapid pace of change (\"One Week\"). The facial expression is serious and wide-eyed. The mood is **dramatic and urgent**.\n",
      "\n",
      "**11:19 - 11:34**: The voice is emphatic and highlights the unprecedented growth and autonomy of AI. The facial expression is serious and impactful. The mood is **profound and awe-inspiring**.\n",
      "\n",
      "**11:34 - 11:46**: The voice is direct and advising, emphasizing the need to adapt to rapid change. The facial expression is serious and determined. The mood is **advisory and urgent**.\n",
      "\n",
      "**11:46 - 11:57**: The voice is insightful and emphasizes learning patterns over specific tools. The facial expression is thoughtful and wise. The mood is **insightful and strategic**.\n",
      "\n",
      "**11:57 - 12:10**: The voice is confident and asserts the new competitive advantage. The facial expression is direct and empowering. The mood is **empowering and visionary**.\n",
      "\n",
      "**12:10 - 12:30**: The voice is clear and concise, summarizing the major updates, maintaining an informative tone. The mood is **summarizing and factual**.\n",
      "\n",
      "**12:30 - 12:41**: The voice is reflective and emphasizes the pattern of economic reorganization. The facial expression is thoughtful and serious. The mood is **reflective and profound**.\n",
      "\n",
      "**12:41 - 12:44**: The voice is calm and conclusive, signing off. The mood is **conclusive and professional**.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)\n",
    "#here we saw the output how our model doing analysis of the video in depth and how generate the detail ouput "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6503c14",
   "metadata": {},
   "outputs": [],
   "source": [
    " # how gemini understand all the frames in depth \n",
    "url_link = 'https://youtu.be/TzGtJfbCGws?si=Ki7HOF6JXnaFmhZP'\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.5-flash',\n",
    "    contents = types.Content(\n",
    "        parts = [types.Part(\n",
    "            file_data = types.FileData(file_uri = url_link )\n",
    "        ),\n",
    "        types.Part(text = 'Extract all visible on screen text from this video , along with the time it appears ')\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1f25b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a comprehensive log of all on-screen text, every event, and each action detected in the video.\n",
      "- **00:00 - 00:01**: OpenAI has\n",
      "- **00:02 - 00:03**: World's most valuable\n",
      "- **00:03 - 00:06**: private company\n",
      "- **00:06 - 00:09**: Worth more than the GDP of 150 countries combined\n",
      "- **00:09 - 00:10**: Perplexity made\n",
      "- **00:11 - 00:13**: Perplexity AI rolls out Comet browser for free worldwide\n",
      "- **00:13 - 00:14**: completely free.\n",
      "- **00:14 - 00:15**: OpenAI also dropped Sora 2.\n",
      "- **00:15 - 00:16**: A social app,\n",
      "- **00:16 - 00:18**: with use cases not restricted to socializing.\n",
      "- **00:18 - 00:20**: Microsoft brought out Vibe working.\n",
      "- **00:20 - 00:21**: Dreamer 4\n",
      "- **00:21 - 00:23**: and Google's AI just learned to mine diamonds in Minecraft\n",
      "- **00:23 - 00:25**: without ever playing the game.\n",
      "- **00:26 - 00:27**: And there is much, much more.\n",
      "- **00:27 - 00:28**: Hello everyone,\n",
      "- **00:28 - 00:29**: I'm Vinayakk Singhal,\n",
      "- **00:29 - 00:30**: and you're watching\n",
      "- **00:30 - 00:31**: THE CUTTING EDGE NEWS\n",
      "- **00:31 - 00:34**: From new breakthroughs to bold experiments.\n",
      "- **00:34 - 00:36**: Here are 12 AI updates shaping the world's biggest headlines.\n",
      "- **00:36 - 00:39**: And trust me, you need to know them all.\n",
      "- **00:39 - 00:42**: It's just been a week and the most crazy one.\n",
      "- **00:42 - 00:44**: So without any further delay, let's start\n",
      "- **00:44 - 00:45**: with OpenAI\n",
      "- **00:45 - 00:46**: and Sora 2.\n",
      "- **00:46 - 00:50**: Today, we're announcing the Sora app, powered by the all-new Sora 2.\n",
      "- **00:50 - 00:54**: Well that may look and sound just like OpenAI CEO Sam Altman,\n",
      "- **00:54 - 00:59**: but it's actually a video generated completely by AI.\n",
      "- **00:59 - 01:00**: It's all powered by Sora 2.\n",
      "- **01:00 - 01:02**: Now the clips are going viral,\n",
      "- **01:02 - 01:05**: like one of CEO Sam Altman shoplifting GPUs.\n",
      "- **01:05 - 01:09**: Please, I really need this for Sora inference. This video is too good.\n",
      "- **01:11 - 01:12**: For those of you who missed the launch of\n",
      "- **01:12 - 01:13**: OpenAI's launch of Sora 2 and its use cases\n",
      "- **01:12 - 01:13**: Sora 2\n",
      "- **01:13 - 01:18**: Sora 2, it's the first AI video generator that creates videos with synchronized audio, accurate physics, and some anime.\n",
      "- **01:18 - 01:19**: Physics with Sora 2\n",
      "- **01:19 - 01:20**: and a built-in social app.\n",
      "- **01:20 - 01:23**: Like TikTok, every video is AI generated.\n",
      "- **01:23 - 01:24**: The standout feature here is Cameos.\n",
      "- **01:24 - 01:25**: Start recording and say the numbers on screen\n",
      "- **01:25 - 01:26**: Start recording\n",
      "- **01:26 - 01:35**: You record yourself once, verify your identity, and then you can insert yourself into any AI scene. Fighting dragons, doing backflips, hanging out in the space with your friends. Everything's possible.\n",
      "- **01:36 - 01:39**: But here's what this technology enables across different domains.\n",
      "- **01:39 - 01:42**: 1. Marketing and content creation is being transformed\n",
      "- **01:42 - 01:44**: his art style to create something new!\n",
      "- **01:44 - 01:48**: Early AI video tools have already shown dramatic productions and Sora 2 takes this further.\n",
      "- **01:48 - 01:56**: Product demos that traditionally needed full camera crews, location shoots and editing teams can now be generated from text prompts.\n",
      "- **01:56 - 02:04**: The physics accuracy means you can show products in motion, fabric flowing, liquids pouring, textures responding to light, all without a physical shoot.\n",
      "- **02:04 - 02:07**: 2. Filmmakers are using it for rapid prototyping\n",
      "- **02:07 - 02:15**: Instead of expensive previews or storyboards, directors can now generate entire scene concepts to test visual ideas before committing to production.\n",
      "- **02:15 - 02:21**: Want to see how a zero gravity fight sequence might look, or a rain scene with specific lighting?\n",
      "- **02:21 - 02:24**: Generate it in minutes, iterate, then shoot the real thing with confidence.\n",
      "- **02:24 - 02:27**: 3. Educators have a new visualization tool\n",
      "- **02:27 - 02:38**: Complex concepts that were previously limited to diagrams or expensive animations can now be brought to life. Physics demonstrations, historical recreations, all can be generated on demand.\n",
      "- **02:38 - 02:46**: Medical education researchers have already published papers exploring how the original Sora could demonstrate surgical procedures without operating rooms.\n",
      "- **02:46 - 02:49**: 4. E-commerce sellers can solve impossible product shoots\n",
      "- **02:49 - 03:02**: Need to show a pet product in action, but your cat won't cooperate? Want food sizzle reels without waste? Need to demonstrate products in environments you can't access? AI video generation removes these physical constraints.\n",
      "- **03:02 - 03:07**: Small businesses can now create professional quality content at a fraction of traditional costs.\n",
      "- **03:07 - 03:09**: Now coming to the insane part.\n",
      "- **03:09 - 03:16**: While OpenAI was launching Sora 2, it officially became the most valuable private company in the world, valued at $500 billion.\n",
      "- **03:16 - 03:17**: Let me put that in perspective.\n",
      "- **03:18 - 03:22**: SpaceX, the number two most valuable private company is valued at $400 billion.\n",
      "- **03:22 - 03:25**: OpenAI just overtook them.\n",
      "- **03:25 - 03:30**: This happened after a recent employee stock sale, where the company authorized employees to sell $10 billion in shares,\n",
      "- **03:30 - 03:34**: though shares worth only $6.6 billion were ultimately sold.\n",
      "- **03:34 - 03:36**: But here's the wild part.\n",
      "- **03:36 - 03:41**: That valuation means OpenAI is worth more than the GDP of 150 countries combined.\n",
      "- **03:41 - 03:43**: Think about that for a second.\n",
      "- **03:43 - 03:49**: A company that's less than 10 years old, that makes a chatbot and some AI tools is now worth half a trillion dollars.\n",
      "- **03:49 - 03:54**: OpenAI will reshape every industry, every workflow, every creative process.\n",
      "- **03:54 - 03:59**: And based on what we are seeing with Sora 2 and the other updates this week, they might just be right.\n",
      "- **03:59 - 04:02**: Speaking of massive moves, Perplexity just made one.\n",
      "- **04:02 - 04:03**: Perplexity's Comet browser is now free for everyone\n",
      "- **04:03 - 04:05**: Remember their Comet browser that cost $200 per month?\n",
      "- **04:05 - 04:07**: They made it completely free.\n",
      "- **04:07 - 04:09**: Comet is an AI powered browser with a built-in assistant\n",
      "- **04:09 - 04:13**: that can actually browse for you. Need to compare insurance plans?\n",
      "- **04:13 - 04:17**: Comet searches, compares and summarizes.\n",
      "- **04:17 - 04:22**: Want to find the cheapest concert tickets? Comet adds them to your cart while you make a sandwich.\n",
      "- **04:22 - 04:27**: The CEO said this can boost productivity so much that companies won't need to hire additional people.\n",
      "- **04:27 - 04:29**: That's a bold claim, but here's what's real.\n",
      "- **04:29 - 04:35**: Users who downloaded Comet increased their question asking by 6 to 18X on day one.\n",
      "- **04:35 - 04:49**: For subscribers of the Max plan, which costs $200 a month, there's now a background assistant feature that works like having a team of assistants. You can tell it to draft your emails, find the best flights, and prepare a meeting brief, all simultaneously in the background.\n",
      "- **04:49 - 04:52**: Why does this matter? Google Chrome has dominated browsers for over a decade.\n",
      "- **04:52 - 04:57**: Now we have AI first browsers competing on intelligence, not just speed.\n",
      "- **04:57 - 05:05**: This is the beginning of AI browser wars, and when a company makes a $200 product free, they're planning for market dominance, not profit.\n",
      "- **05:05 - 05:07**: Now here is where things get really interesting.\n",
      "- **05:07 - 05:09**: Google DeepMind just published a paper on Dreamer 4,\n",
      "- **05:09 - 05:12**: and this is genuinely groundbreaking.\n",
      "- **05:12 - 05:22**: Dreamer 4 became the first AI agent to mine diamonds in Minecraft using only offline data. It never played the actual game during training, instead it learned by imagining.\n",
      "- **05:22 - 05:23**: Think about what this means.\n",
      "- **05:23 - 05:36**: The AI watched 2500 hours of Minecraft videos, built an internal world model, then practiced inside its own imagination until it could execute 20,000 plus consecutive actions to reach diamonds.\n",
      "- **05:36 - 05:40**: Dreamer 4 managed to mine diamonds in about 0.7% of its test runs.\n",
      "- **05:40 - 05:46**: That might sound small, but it's still ahead of OpenAI's VPT agent, an earlier AI trained on thousands of hours of Minecraft videos to imitate human players,\n",
      "- **05:46 - 05:51**: which could not reach that milestone.\n",
      "- **05:51 - 05:58**: Even more impressive, Dreamer 4 runs smoothly on a single GPU, far faster than older models.\n",
      "- **05:58 - 06:12**: And this matters beyond gaming, because this is exactly how we'll train robots in the future. Instead of expensive real-world testing, robots will practice millions of scenarios in imagination, then execute in reality. This is the part to general purpose robotics.\n",
      "- **06:12 - 06:15**: Anthropic just integrated Claude directly into Slack.\n",
      "- **06:15 - 06:17**: This is huge news for enterprise workflows.\n",
      "- **06:17 - 06:24**: You can now mention Claude in any Slack thread as if a real person. DM it privately or use the AI assistant panel.\n",
      "- **06:24 - 06:34**: Coming to the smart part, they also let you connect Slack to Claude. So when you're doing deep research in Claude, it can pull context from your Slack conversations and honestly, its use cases are wild.\n",
      "- **06:34 - 06:37**: First it can prepare you for meetings by pulling relevant discussions.\n",
      "- **06:37 - 06:44**: Second it can create documentation from scattered conversations. And third, it can even draft your responses to important messages.\n",
      "- **06:44 - 06:49**: All while respecting your permission settings because it can only see what you see.\n",
      "- **06:49 - 06:54**: For team and enterprise plans, this will transform how companies work.\n",
      "- **06:54 - 07:00**: One team reported cutting onboarding time from months to weeks because Claude could instantly summarize the entire company's Slack history or any topic.\n",
      "- **07:00 - 07:04**: Microsoft just announced Vibe Working.\n",
      "- **07:04 - 07:06**: Their answer to Vibe Coding, but only for office apps.\n",
      "- **07:06 - 07:20**: Agent Mode in Excel can now run complete financial analysis, create visualizations, and validate results iteratively. You just say analyze the sales data and make it visual, and it decides which formulas to use, creates new sheets, and even builds charts.\n",
      "- **07:20 - 07:24**: In Microsoft Word, Agent Mode turns writing into conversation.\n",
      "- **07:24 - 07:34**: Tell it update this monthly report into latest data from the September email, and it handles everything. Finding the email, updating tables, and even fixing the format.\n",
      "- **07:34 - 07:36**: But here is the twist.\n",
      "- **07:36 - 07:43**: The office agent in Copilot uses Anthropic's Claude to create PowerPoint and Word docs, while Agent Mode uses OpenAI's models.\n",
      "- **07:43 - 07:46**: Microsoft is hedging its best with multiple AI providers.\n",
      "- **07:46 - 07:54**: The accuracy, Agent Mode scored 57.2% on spreadsheet benchmarks compared to 71.3% for human beings.\n",
      "- **07:54 - 07:56**: Not perfect, but improving fast.\n",
      "- **07:56 - 07:59**: Nothing's Essential platform just dropped, and this is wild.\n",
      "- **07:59 - 08:14**: The smartphone company launched an AI platform that lets you create apps just by describing them. Want an app that captures receipts from your camera roll and exports a finance ready PDF every Friday? Just say it. Want a mood tracker that syncs with a music playlist? Done.\n",
      "- **08:14 - 08:24**: It generates these as widgets. You can add to your home screen instantly. And they're calling it the Nothing Playground, a community hub where you can share, remix, and download apps created by others.\n",
      "- **08:24 - 08:32**: CEO Carl Pei says that this is the first step towards building an entirely AI native operating system by 2027.\n",
      "- **08:32 - 08:38**: Currently available only on Nothing phones, but this challenges the entire concept of App Stores.\n",
      "- **08:38 - 08:41**: Why download pre-made apps when AI can build exactly what you need?\n",
      "- **08:41 - 08:43**: Another very interesting news.\n",
      "- **08:43 - 08:50**: Toyota just opened its $10 billion Woven City at the base of Mount Fuji in Japan, and 300 residents are already living there as of October 2025.\n",
      "- **08:50 - 08:57**: This is not just a simulation, it's a real town built to test future technology in everyday life.\n",
      "- **08:57 - 09:06**: The streets are split into three zones. One for autonomous vehicles, one for personal mobility like e-bikes, and one for purely pedestrians.\n",
      "- **09:06 - 09:10**: All deliveries run through underground tunnels carved into volcanic rocks.\n",
      "- **09:10 - 09:19**: Residents, mostly Toyota employees and researchers, are testing self-driving shuttles, delivery robots, smart homes with AI health monitoring and hydrogen powered everything.\n",
      "- **09:19 - 09:28**: But here's the key: it's not just about the tech, it's about how homes, mobility, logistics and energy systems connect and evolve based on how people actually live.\n",
      "- **09:28 - 09:32**: Toyota invested $10 billion knowing this might never make them money.\n",
      "- **09:32 - 09:40**: But they're betting that learning how humans interact with autonomous systems in real life, not in labs, but while living, walking, working, that data is priceless.\n",
      "- **09:40 - 09:47**: This is the first place where you can actually live in the future being tested. Not on paper, but in reality.\n",
      "- **09:47 - 09:52**: Just thinking about living in such a place in India gives me goosebumps. Tell me what you think about it.\n",
      "- **09:52 - 09:57**: Now it's time to shift gears for a quick rapid fire round up of this week's biggest AI updates.\n",
      "- **09:57 - 10:04**: Notebook LM got customizable chats and a learning guide feature which will test and deepen your understanding of your materials.\n",
      "- **10:04 - 10:08**: Notion launched Map View. Finally, a spatial way to organize your projects.\n",
      "- **10:08 - 10:15**: xAI is building Grokipiedia, basically Wikipedia powered by Grok AI for real-time, up-to-date information.\n",
      "- **10:15 - 10:27**: Google researchers released PASTA, an AI system that adapts to individual creative preferences through repeated interactions, learning what visual styles users actually want rather than requiring complex prompt engineering.\n",
      "- **10:27 - 10:36**: Tencent's open source Hunyuan Image 3.0 model moved to first place on LMArena's text to image leaderboard, surpassing top closed options.\n",
      "- **10:36 - 10:44**: Anthropic published research detailing Claude 4.5 Sonnet's cybersecurity capabilities, with the new model achieving top marks on industry benchmarks.\n",
      "- **10:44 - 10:50**: OpenAI acquired personal finance startup Roy with CEO Sujith Vishwajit joining as the AI leader in the deal.\n",
      "- **10:50 - 10:57**: Now here's what connects all of this. We're witnessing the fastest wealth creation in human history, alongside the fastest technology deployment.\n",
      "- **10:57 - 11:01**: OpenAI hits $500 billion valuation.\n",
      "- **11:01 - 11:02**: They used that to build Sora 2.\n",
      "- **11:02 - 11:05**: Sora 2 creates a new social platform.\n",
      "- **11:05 - 11:10**: Now we're moving from AI as a tool to AI as a collaborator to AI as an autonomous worker.\n",
      "- **11:10 - 11:14**: And here's what nobody is talking about. Every single one of these updates happened in one week.\n",
      "- **11:14 - 11:18**: Not one quarter, not one year, one week.\n",
      "- **11:18 - 11:34**: When a technology company becomes worth more than most country's economies in less than a decade, and when that same week they release tools that can imagine, browse, collaborate and create autonomously, we're not watching incremental progress anymore, we're watching exponential transformation.\n",
      "- **11:34 - 11:36**: So what should you do?\n",
      "- **11:36 - 11:46**: First, stop waiting for things to stabilize. They won't. The companies betting half a trillion dollars on AI aren't playing for this to slow down, they're planning for it to accelerate.\n",
      "- **11:46 - 11:57**: Second, focus on learning patterns, not specific tools. The tools will change every now and then, but understanding how AI thinks, what it's good at, and how to direct it, that's the lasting skill.\n",
      "- **11:57 - 12:10**: Third, remember when AI makes a company worth $500 billion and enables tools that work autonomously, your competitive advantage isn't knowing more than AI, it's knowing what to build with AI that nobody else has thought of yet.\n",
      "- **12:10 - 12:18**: This week OpenAI became the world's most valuable private company at $500 billion valuation, worth more than 150 countries combined.\n",
      "- **12:18 - 12:21**: They launched Sora 2 with social features and accurate physics.\n",
      "- **12:21 - 12:23**: Perplexity made Comet browser free.\n",
      "- **12:23 - 12:26**: Dreamer 4 mastered Minecraft through imagination.\n",
      "- **12:26 - 12:28**: Claude integrated with Slack.\n",
      "- **12:28 - 12:33**: Microsoft introduced Vibe Working and about six other major updates.\n",
      "- **12:33 - 12:40**: The pattern, we are not just seeing AI improve, we are seeing the entire global economy reorganize around it at unprecedented speed.\n",
      "- **12:40 - 12:43**: This is Vinayak, and you're watching The Cutting Edge News.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a341b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can compare multiple videos \n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='models/gemini-2.5-flash',\n",
    "    contents=types.Content(\n",
    "        parts=[\n",
    "            types.Part(\n",
    "                file_data=types.FileData(file_uri='https://youtu.be/RdVyjKRxZfs?si=72BsK9AnpssB29Q2')\n",
    "            ),\n",
    "            types.Part(\n",
    "                file_data=types.FileData(file_uri='https://youtu.be/yeHNPnfiYcA?si=M94UJhEuVdcrqt0s')\n",
    "            ),\n",
    "            types.Part(text=\"Compare these two videos. List the similarities, differences, and any matching scenes.\")\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4310bcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two videos are similar in format, with an instructor-led presentation accompanied by visual aids and occasional cut-scenes. Both videos address practical advice for an Indian audience, focusing on financial planning in the first video and career guidance in the second.\n",
      "\n",
      "Here's a breakdown of similarities, differences, and matching scenes:\n",
      "\n",
      "**Similarities:**\n",
      "\n",
      "*   **Format:** Both videos feature a male instructor speaking directly to the camera, intercut with animated or graphical explanations and occasional stock footage/movie clips.\n",
      "*   **Instructional Tone:** Both videos aim to educate the viewer on specific topics (finance and career) and provide actionable advice.\n",
      "*   **Language:** Both videos are presented in Hindi, targeting an Indian audience.\n",
      "*   **Visual Aids:** Both instructors use a whiteboard/large paper to write down key points and examples, making the information easier to follow.\n",
      "*   **Encouragement for Engagement:** Both videos conclude with a call to action for comments and likes, and emphasize the value of continuous learning.\n",
      "\n",
      "**Differences:**\n",
      "\n",
      "*   **Topic:** The first video (00:00-01:05:41) focuses on **financial rules and planning**, covering car buying, house buying, insurance, investment strategies, and budgeting. The second video (00:00-00:08:54) focuses on the **changing landscape of tech hiring and career development**, particularly for entry-level software engineers.\n",
      "*   **Instructor:** While both instructors are male, they are different individuals with distinct presenting styles.\n",
      "*   **Visual Aesthetics (Main Set):** The first video has a more stylized set with specific lighting (a large, round light on the left) and a large screen displaying nature scenes in the background. The second video's set is more office-like, with wooden panels and bookshelves.\n",
      "*   **Specific Rules/Advice:** The content of the advice is entirely different, tailored to their respective topics. For example, the first video discusses the \"20/4/10-50 rule\" for car buying, while the second discusses the importance of Cloud, DevOps, and AI for interns.\n",
      "*   **Tone/Urgency:** The second video has a slightly more urgent and critical tone when discussing the current challenges in the tech job market (\"New norm in Tech hiring,\" \"Why is the Tech Market like this?\").\n",
      "\n",
      "**Matching Scenes:**\n",
      "\n",
      "*   **00:00 - 00:00:09 (Video 1) vs. 00:00 - 00:00:13 (Video 2):** Both videos start with the instructor directly addressing the audience, setting the stage for the topic to be discussed. They both use hand gestures to emphasize their points.\n",
      "*   **00:10 - 00:16 (Video 1):** The instructor points to a list of rules written on a virtual whiteboard, providing an overview of what will be covered. This is a common setup also seen in the second video's explanation style.\n",
      "*   **00:50 - 00:58 (Video 1):** The instructor uses a physical whiteboard to write down the \"20/4/10-50\" car buying rule. This visual aid technique is used throughout the first video and is similar in concept to how the second video presents its key points via text on screen or a physical whiteboard.\n",
      "*   **00:59 - 01:00 (Video 1) & 03:32 - 03:33 (Video 1) vs. 00:03 - 00:06 (Video 2):** Both videos use stock footage or animated clips to illustrate points. The first video shows an animated family with money falling, then people looking at cars, and a family entering a house. The second video shows people working on laptops in a collaborative environment.\n",
      "*   **02:29 - 02:34 (Video 1) & 03:25 - 03:31 (Video 1) & 07:04 - 07:12 (Video 1):** The instructor uses hand gestures, including the \"OK\" sign and open palms, to emphasize understanding or agreement. The second video's instructor also frequently uses similar hand gestures.\n",
      "*   **08:24 - 08:29 (Video 1) vs. 00:10 - 00:12 (Video 2):** Both instructors make a point about how quickly things change over time (\"with time\" vs. \"aaj ekdam change ho gaya hai\").\n",
      "*   **08:44 - 08:55 (Video 1) vs. 00:14 - 00:20 (Video 2) & 00:25 - 00:42 (Video 2) & 02:07 - 02:11 (Video 2):** Both videos use screen recordings of job descriptions/insurance websites to show real-world examples.\n",
      "*   **10:09 - 10:16 (Video 1) vs. 00:07 - 00:10 (Video 2):** Both videos feature a scene with a mobile phone screen displaying relevant content (Emergency Fund video in Video 1, coding in Video 2).\n",
      "*   **11:42 - 11:47 (Video 1) & 02:29 - 02:34 (Video 1):** The instructor emphasizes certain points as \"thumb rules\" or \"guiding principles,\" advising viewers to adapt them to their personal situations. The second video also encourages applying concepts with context.\n",
      "*   **15:32 - 15:47 (Video 1) vs. 02:44 - 02:56 (Video 2):** Both videos feature the instructor encouraging viewers to comment their progress or thoughts.\n",
      "*   **16:06 - 16:15 (Video 1) vs. 02:51 - 02:57 (Video 2):** Both videos end with a concluding remark, a call to action (like/subscribe), and a final positive affirmation.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da74c82",
   "metadata": {},
   "source": [
    "Audio Understanding Capability of Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video understanding = vision + audio combined.\n",
    "# Audio understanding = focuses purely on sound (speech, music, noises) without visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ddf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini can analyze and understand audio input, enabling use cases like the following:\n",
    "    # Describe, summarize, or answer questions about audio content.\n",
    "    # Provide a transcription of the audio.\n",
    "    # Analyze specific segments of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e63edd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Provide me transcript  of the speech  from 0:30 to 02:30\"\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=[my_file , prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfe88ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And then what is life's good? That's nothing but the slogan of their company. Well, is life really good or not everyone has to check that for themselves. But is the IPO good or not? If you want to know the answer for this, you'll have to check out the full video. And just in case if you feel that I know a lot about this company, just check if you know the answers to these three questions. Number one, what will be the impact of Trump tariffs on the company? Number two, has LG gained market share or lost market share in the last few years? And number three, is the company overvalued or undervalued? Before we move on, a quick update on my Har Ghar Investor Hyderabad event. I'm getting so many requests to extend the early bird offer. So, finally, today will be the last day for the early bird offer. If you check, a lot of seats have already been booked. In this seminar, I'm going to specifically focus on how AI can be used on fundamental analysis. I'm going to talk about my NIFTY target. And very important, in this entire seminar, there's going to be a separate section which is going to be a Q&A section where you can ask your questions directly to me. Seats are filling up fast. Early bird offer ends today. So, hurry up and check out the link in the pinned comment and in the description box below. All right, now let's get started with the basics as to what does the company do, right? LG Electronics India Limited is mainly engaged in what? Manufacturing and sale of what? Of home appliances, air solutions and home entertainment. But I'm sure you might be like, what all things are included in this? Now, if I look at the first point, which is home appliances and air solutions. It includes something like refrigerators, washing machines, air conditioners, water purifiers, air purifiers, dishwashers, microwave ovens, compressors, motors, whatever, right? So, this is simple. Home appliances and air solutions, of course, it will be something like air conditioner, air purifier, whatever. Now, if I go to the second segment, it's home entertainment. And in that, it's mainly televisions and audio devices. And if it's about a B2B business, then it's about projectors, monitors, and all that as well. So, I hope you have understood two things here. They have two product segments, two product lines. One is home appliances and air solutions in which we have something like uh washing machines, refrigerators, air conditioners, microwave ovens, whatever. And in home entertainment\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
